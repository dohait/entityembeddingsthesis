{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random as tf_random\n",
    "from random import seed\n",
    "from numpy.random import seed as np_seed\n",
    "def random_seed(seed_value):\n",
    "    np_seed(seed_value) # cpu vars\n",
    "    seed(seed_value) # Python\n",
    "    tf_random.set_seed(seed_value) #TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'sample_perc':1,\n",
    "          'bs': 512,\n",
    "          'epochs': 50, \n",
    "          'n_folds': 5,\n",
    "          'optimizer': 'sgd',\n",
    "          'lr': 1e-01,\n",
    "          'momentum': 0.9,\n",
    "          'wd': 0.01,\n",
    "          'emb_szs': {'hips': 5, 'cup_size': 5, 'user_name': 50, 'item_id': 50, 'category': 5, 'length': 5},\n",
    "          'trainable': False\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import pprint\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rc('figure', figsize = (20, 8))\n",
    "plt.rc('font', size = 14)\n",
    "plt.rc('axes.spines', top = False, right = False)\n",
    "plt.rc('axes', grid = False)\n",
    "plt.rc('axes', facecolor = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'clothing_fit_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bust_values(x):\n",
    "    \"\"\"Function to normalize the bust values\"\"\"\n",
    "    try:\n",
    "        if pd.notnull(x):\n",
    "            if \"-\" in x:\n",
    "                assert len(x.split(\"-\")) == 2\n",
    "                return np.mean([int(num) for num in x.split(\"-\")])\n",
    "            else:\n",
    "                return int(x) #Can throw an exception\n",
    "    except Exception as e: \n",
    "        # For anykind of exception return None\n",
    "        return None\n",
    "    return None\n",
    "  \n",
    "def normalize_height_values(x):\n",
    "    \"\"\"Function to normalize the height values\"\"\"\n",
    "    if pd.notnull(x):\n",
    "        try: \n",
    "            return (int(x[0])*30.48) + (int(x[4:-2])*2.54)\n",
    "        except:\n",
    "            return (int(x[0])*30.48) # there can't be a 10ft+ customer,neither was observed\n",
    "    return None\n",
    "\n",
    "def preprocess_data(sample_perc=1): \n",
    "    modcloth_df = pd.read_json((data_dir + 'modcloth_data.json'), lines=True)\n",
    "\n",
    "    # Changing column names and removing unnecessary spaces\n",
    "    modcloth_df.columns = [x.replace(\" \",\"_\") for x in modcloth_df.columns]\n",
    "\n",
    "    #normalize bust and height\n",
    "    modcloth_df[\"bust\"] = modcloth_df[\"bust\"].apply(lambda x: normalize_bust_values(x))\n",
    "    modcloth_df[\"height\"] = modcloth_df[\"height\"].apply(lambda x: normalize_height_values(x))\n",
    "\n",
    "    #remove outliers\n",
    "    Q1 = modcloth_df.quantile(0.25)\n",
    "    Q3 = modcloth_df.quantile(0.75)\n",
    "    Q1.drop([\"item_id\",\"user_id\"], inplace =True)\n",
    "    Q3.drop([\"item_id\",\"user_id\"], inplace = True)\n",
    "    IQR = Q3 - Q1\n",
    "    modcloth_df = modcloth_df[~((modcloth_df < (Q1 - 1.5 * IQR)) |(modcloth_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    modcloth_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # handling missing values\n",
    "    modcloth_df.drop([\"waist\", \"bust\", \"shoe_width\", \"shoe_size\"], axis= 1, inplace=True)\n",
    "    bra_size_to_cup_size = {x:y for x,y in modcloth_df.groupby(\"bra_size\")[\"cup_size\"].agg(pd.Series.mode).reset_index().values}\n",
    "    cup_size_to_bra_size = {x:y for x,y in modcloth_df.groupby(\"cup_size\")[\"bra_size\"].median().reset_index().values}\n",
    "    bra_size_med = modcloth_df.bra_size.median() \n",
    "    cup_size_mod = bra_size_to_cup_size[bra_size_med] # not matching with cup size mode\n",
    "    imputed_value = []\n",
    "    for x,y in zip(modcloth_df[\"bra_size\"],modcloth_df[\"cup_size\"]):\n",
    "        if pd.isnull(x) and pd.isnull(y):\n",
    "            imputed_value.append([bra_size_med, cup_size_mod])\n",
    "        elif pd.isnull(x) and pd.notnull(y):\n",
    "            imputed_value.append([cup_size_to_bra_size[y], y])\n",
    "        elif pd.notnull(x) and pd.isnull(y):\n",
    "            imputed_value.append([x, bra_size_to_cup_size[x]])\n",
    "        else:\n",
    "            imputed_value.append([x,y])\n",
    "    modcloth_df[[\"bra_size\",\"cup_size\"]] = imputed_value\n",
    "    modcloth_df.review_summary.fillna(\"Unknown\", inplace=True)\n",
    "    modcloth_df.review_text.fillna(\"Unknown\", inplace=True)\n",
    "    modcloth_df[\"length\"] = modcloth_df.length.fillna(modcloth_df['length'].value_counts().index[0])\n",
    "    modcloth_df.hips.fillna(-1.0, inplace = True)\n",
    "    bins = [-2,0,31,37,40,44,75]\n",
    "    labels = ['Unknown','XS','S','M', 'L','XL']\n",
    "    modcloth_df.hips = pd.cut(modcloth_df.hips, bins, labels=labels)\n",
    "\n",
    "    #drop unreliable or double included variables\n",
    "    modcloth_df.drop('review_summary', axis=1, inplace = True)\n",
    "    modcloth_df.drop('user_id', axis=1, inplace = True)\n",
    "    modcloth_df[\"user_name\"] = modcloth_df.user_name.apply(lambda x: x.lower())\n",
    "    \n",
    "    #impute missing values for height and quality variables\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "    median_imputer = median_imputer.fit(modcloth_df[['height','quality']])\n",
    "    modcloth_df[['height','quality']] = median_imputer.transform(modcloth_df[['height','quality']])\n",
    "    \n",
    "    if sample_perc < 1:\n",
    "        #Use a subset of the data\n",
    "        modcloth_df = modcloth_df[modcloth_df['review_text'] != 'Unknown'].reset_index(drop=True)\n",
    "        np.random.seed(123)\n",
    "        sample = np.random.permutation(int(sample_perc * len(modcloth_df.index)))\n",
    "        modcloth_df = modcloth_df.loc[sample].reset_index(drop=True)\n",
    "        print(f'Using {sample_perc*100}% of the data')\n",
    "        \n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        random_seed(123)\n",
    "        strat_sampling = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "        train_indexes, test_indexes = next(strat_sampling.split(modcloth_df.loc[:,modcloth_df.columns!='fit'], modcloth_df['fit']))\n",
    "    else:\n",
    "        #use complete data with fixed train and test set (self splitted)\n",
    "        from numpy import genfromtxt\n",
    "        train_indexes = genfromtxt(data_dir + 'train_indexes.csv', delimiter=',', dtype=int)\n",
    "        test_indexes = genfromtxt(data_dir + 'test_indexes.csv', delimiter=',', dtype=int)\n",
    "        \n",
    "    return (modcloth_df, train_indexes, test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__ #used tensorflow 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modcloth_df, train_indexes, test_indexes = preprocess_data(params['sample_perc'])\n",
    "modcloth_df.shape, train_indexes.shape, test_indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set\n",
    "train = modcloth_df.loc[train_indexes]\n",
    "test = modcloth_df.loc[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocate variables to data types and input pathway\n",
    "user_categorical_features = [\"user_name\",\"hips\",\"cup_size\"]\n",
    "user_numerical_features = [\"height\",\"bra_size\"]\n",
    "item_categorical_features = [\"item_id\", \"category\", \"length\"]\n",
    "item_numerical_features = [\"size\",\"quality\"]\n",
    "dep_name = ['fit']\n",
    "\n",
    "nlp_features = ['review_text']\n",
    "\n",
    "all_features = user_categorical_features + user_numerical_features + item_categorical_features + item_numerical_features + nlp_features\n",
    "\n",
    "CLASS_LABELS =  np.array([\"fit\",\"small\",\"large\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler().fit(train[user_numerical_features + item_numerical_features])\n",
    "train[user_numerical_features + item_numerical_features] = scaler.transform(train[user_numerical_features + item_numerical_features])\n",
    "test[user_numerical_features + item_numerical_features] = scaler.transform(test[user_numerical_features + item_numerical_features])\n",
    "\n",
    "# Categorify categorical features\n",
    "for col in user_categorical_features + item_categorical_features:\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    modcloth_df[col] = modcloth_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle:bool=True, batch_size=32):\n",
    "    '''Create tf.data dataset from pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: pandas DataFrame\n",
    "    shuffle: Boolean indicating whether to shuffle the data\n",
    "    batch_size: batch size\n",
    "    \n",
    "    Returns a tf.data dataset\n",
    "    '''\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('fit')\n",
    "    #create one-hot encoded label vector\n",
    "    labels = labels.apply(lambda x:x == CLASS_LABELS)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    if batch_size == None:\n",
    "        # complete data in 1 batch\n",
    "        ds = ds.batch(len(labels))\n",
    "    else:\n",
    "        ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature columns and the final input for the neural network with separated input pathways\n",
    "\n",
    "# Numeric Columns\n",
    "numeric_users = {\n",
    "    col : feature_column.numeric_column(col) \\\n",
    "          for col in user_numerical_features\n",
    "}\n",
    "\n",
    "numeric_items = {\n",
    "    col : feature_column.numeric_column(col) \\\n",
    "          for col in item_numerical_features\n",
    "}\n",
    "\n",
    "\n",
    "# Categorical Columns\n",
    "hips = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'hips', modcloth_df.hips.unique().tolist())\n",
    "cup_size = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'cup_size', modcloth_df.cup_size.unique().tolist())\n",
    "user_name = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'user_name', modcloth_df.user_name.unique().tolist())\n",
    "\n",
    "item_id = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'item_id', modcloth_df.item_id.unique().tolist())\n",
    "category = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'category', modcloth_df.category.unique().tolist())\n",
    "length = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'length', modcloth_df.length.unique().tolist())\n",
    "\n",
    "\n",
    "random_seed(123)\n",
    "\n",
    "#uncomment this section (and comment section below) to calculate the dimensions with fastai rule\n",
    "'''\n",
    "#dimensions calculated by fastai rule\n",
    "print('Entity Embedding Dimensions:')\n",
    "emb_szs = {}\n",
    "for column in modcloth_df[user_categorical_features + item_categorical_features]:\n",
    "    n_cat = modcloth_df[column].nunique()\n",
    "    emb_sz = min(fastai_emb_limit, round(1.6 * n_cat**0.56))\n",
    "    emb_szs[column] = emb_sz\n",
    "    print(f'{column}: {n_cat} / {emb_sz}')\n",
    "params['emb_szs'] = emb_szs\n",
    "    \n",
    "hips_embedding = feature_column.embedding_column(hips, dimension=emb_szs['hips'])\n",
    "cup_size_embedding = feature_column.embedding_column(cup_size, dimension=emb_szs['cup_size'])\n",
    "user_name_embedding = feature_column.embedding_column(user_name, dimension=emb_szs['user_name'])\n",
    "\n",
    "item_id_embedding = feature_column.embedding_column(item_id, dimension=emb_szs['item_id'])\n",
    "category_embedding = feature_column.embedding_column(category, dimension=emb_szs['category'])\n",
    "length_embedding = feature_column.embedding_column(length, dimension=emb_szs['length'])\n",
    "'''\n",
    "\n",
    "#uncomment this section (and comment section above) to use dimensions of practitioner\n",
    "\n",
    "#Practitioner embeddings\n",
    "hips_embedding = feature_column.embedding_column(hips, dimension=5)\n",
    "cup_size_embedding = feature_column.embedding_column(cup_size, dimension=5)\n",
    "user_name_embedding = feature_column.embedding_column(user_name, dimension=50)\n",
    "\n",
    "item_id_embedding = feature_column.embedding_column(item_id, dimension=50)\n",
    "category_embedding = feature_column.embedding_column(category, dimension=5)\n",
    "length_embedding = feature_column.embedding_column(length, dimension=5)\n",
    "\n",
    "\n",
    "\n",
    "cat_users = {\n",
    "    'hips' : hips_embedding,\n",
    "    'cup_size' : cup_size_embedding,\n",
    "    'user_name': user_name_embedding\n",
    "}\n",
    "\n",
    "cat_items = {\n",
    "    'item_id' : item_id_embedding,\n",
    "    'category' : category_embedding,\n",
    "    'length': length_embedding\n",
    "}\n",
    "\n",
    "\n",
    "#prepare final inputs\n",
    "input_user = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_users.keys()\n",
    "}\n",
    "input_user.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in cat_users.keys()\n",
    "})\n",
    "\n",
    "input_items = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype = 'float32') \\\n",
    "          for colname in numeric_items.keys()\n",
    "}\n",
    "\n",
    "input_items.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in cat_items.keys()\n",
    "})\n",
    "\n",
    "input_nlp = tf.keras.layers.Input(name='review_text', shape=(), dtype = tf.string)\n",
    "\n",
    "#capture all feature columns of the respective pathway in one vector\n",
    "feat_cols_user = list(numeric_users.values()) + list(cat_users.values())\n",
    "feat_cols_item = list(numeric_items.values()) + list(cat_items.values())\n",
    "\n",
    "# Create feature layers\n",
    "feature_layer_users = keras.layers.DenseFeatures(feat_cols_user)(input_user)\n",
    "feature_layer_items = keras.layers.DenseFeatures(feat_cols_item)(input_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature columns and the final input for the neural network with NOT separated input pathways\n",
    "\n",
    "# Numeric Columns\n",
    "numeric_columns = {\n",
    "    col : feature_column.numeric_column(col) \\\n",
    "          for col in user_numerical_features + item_numerical_features\n",
    "}\n",
    "\n",
    "# Categorical Columns\n",
    "hips = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'hips', modcloth_df.hips.unique().tolist())\n",
    "cup_size = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'cup_size', modcloth_df.cup_size.unique().tolist())\n",
    "user_name = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'user_name', modcloth_df.user_name.unique().tolist())\n",
    "item_id = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'item_id', modcloth_df.item_id.unique().tolist())\n",
    "category = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'category', modcloth_df.category.unique().tolist())\n",
    "length = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'length', modcloth_df.length.unique().tolist())\n",
    "\n",
    "\n",
    "random_seed(123)\n",
    "#one-hot encoded: uncomment this section (and comment section below) to use one-hot encoding\n",
    "'''\n",
    "hips_one_hot = feature_column.indicator_column(hips)\n",
    "cup_size_one_hot = feature_column.indicator_column(cup_size)\n",
    "user_name_one_hot = feature_column.indicator_column(user_name)\n",
    "item_id_one_hot = feature_column.indicator_column(item_id)\n",
    "category_one_hot = feature_column.indicator_column(category)\n",
    "length_one_hot = feature_column.indicator_column(length)\n",
    "\n",
    "cat_columns = {\n",
    "    'hips' : hips_one_hot,\n",
    "    'cup_size' : cup_size_one_hot,\n",
    "    'user_name': user_name_one_hot,\n",
    "    'item_id' : item_id_one_hot,\n",
    "    'category' : category_one_hot,\n",
    "    'length': length_one_hot\n",
    "}\n",
    "'''\n",
    "\n",
    "#embeddings: uncomment this section (and comment section above) to use entity embeddings\n",
    "hips_embedding = feature_column.embedding_column(hips, dimension=5)\n",
    "cup_size_embedding = feature_column.embedding_column(cup_size, dimension=5)\n",
    "user_name_embedding = feature_column.embedding_column(user_name, dimension=50)\n",
    "item_id_embedding = feature_column.embedding_column(item_id, dimension=50)\n",
    "category_embedding = feature_column.embedding_column(category, dimension=5)\n",
    "length_embedding = feature_column.embedding_column(length, dimension=5)\n",
    "\n",
    "cat_columns = {\n",
    "    'hips' : hips_embedding,\n",
    "    'cup_size' : cup_size_embedding,\n",
    "    'user_name': user_name_embedding,\n",
    "    'item_id' : item_id_embedding,\n",
    "    'category' : category_embedding,\n",
    "    'length': length_embedding\n",
    "}\n",
    "\n",
    "\n",
    "#prepare final inputs\n",
    "input_tab = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_columns.keys()\n",
    "}\n",
    "input_tab.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in cat_columns.keys()\n",
    "})\n",
    "\n",
    "#capture all feature columns in one vector\n",
    "feat_cols = list(numeric_columns.values()) + list(cat_columns.values())\n",
    "\n",
    "# Create a feature layer\n",
    "feature_layer_tab = keras.layers.DenseFeatures(feat_cols)(input_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipCon(keras.layers.Layer):\n",
    "    def __init__(self, size, reduce = True, deep = 3, skip_when=0, activation=\"relu\", **kwargs):\n",
    "        \"\"\"\n",
    "        Class for skip connections\n",
    "        \n",
    "        @Params\n",
    "        size = size of dense layer\n",
    "        deep = the depth of network in one SkipCon block call\n",
    "        skip_when =  if a skip connection is required, pass 1\n",
    "        activation = by default using relu\n",
    "        \"\"\"    \n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation) # used to combine skip connections and cascaded dense layers\n",
    "        self.main_layers = []\n",
    "        self.skip_when = skip_when #to be used in call as a control\n",
    "        if reduce:\n",
    "            for _ in range(deep):\n",
    "                self.main_layers.extend([\n",
    "                      keras.layers.Dense(size, activation=activation, \n",
    "                                          use_bias=True),\n",
    "                      keras.layers.BatchNormalization()])\n",
    "\n",
    "                # Reduce the input size by two each time, if the\n",
    "                # network is to be designed deeper and narrow\n",
    "                size = size/2\n",
    "        else:\n",
    "            for _ in range(deep):\n",
    "                self.main_layers.extend([\n",
    "                    keras.layers.Dense(size, activation=activation, use_bias=True),\n",
    "                    keras.layers.BatchNormalization()])\n",
    "\n",
    "        self.skip_layers = []\n",
    "        if skip_when > 0:\n",
    "            if reduce:\n",
    "                size = size*2 # since the size of skipped connection  \n",
    "                                  # should match with cascaded dense\n",
    "            self.skip_layers = [\n",
    "                    keras.layers.Dense(size, activation=activation,use_bias=True),\n",
    "                    keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        if not self.skip_when:\n",
    "            return self.activation(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare evaluation metrics\n",
    "METRICS = [keras.metrics.TruePositives(name='tp'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.TrueNegatives(name='tn'),\n",
    "           keras.metrics.FalseNegatives(name='fn'), \n",
    "           keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "           keras.metrics.CategoricalAccuracy(name='cat_accuracy'),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),\n",
    "    ]\n",
    "\n",
    "def get_optimizer():\n",
    "    '''Function that returns an optimzer based on the parameters for the model'''\n",
    "    \n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(lr=params['lr'], momentum=params['momentum'], decay=params['wd'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=params['lr'])\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=params['lr'], momentum=params['momentum'])\n",
    "    else:\n",
    "        raise Exception('Wrong input for optimizer parameter given.')\n",
    "    return optimizer\n",
    "\n",
    "#early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        verbose=1,\n",
    "        patience=5,\n",
    "        mode='min',\n",
    "        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separated neural networks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_model():\n",
    "    '''Function that creates and compiles the baseline neural network (categorical, numeric + separated pathways)'''\n",
    "    #Customer pathway\n",
    "    user_layer = keras.layers.Dense(256, activation='relu', use_bias = True)(feature_layer_users)\n",
    "    user_layer = SkipCon(size = 256, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(user_layer)\n",
    "    user_layer = keras.layers.Dropout(0.5)(user_layer) \n",
    "    user_layer = SkipCon(size = 256, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(user_layer)\n",
    "    user_layer = keras.layers.Dropout(0.5)(user_layer)\n",
    "    user_layer = SkipCon(size = 64, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(user_layer)\n",
    "\n",
    "    #Item pathway\n",
    "    item_layer = keras.layers.Dense(256, activation='relu', use_bias = True)(feature_layer_items)\n",
    "    item_layer = SkipCon(size = 256, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(item_layer)\n",
    "    item_layer = keras.layers.Dropout(0.5)(item_layer) \n",
    "    item_layer = SkipCon(size = 256, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(item_layer)\n",
    "    item_layer = keras.layers.Dropout(0.5)(item_layer)\n",
    "    item_layer = SkipCon(size = 64, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(item_layer)\n",
    "\n",
    "    # combine the output of the two branches\n",
    "    combined = tf.concat([user_layer, item_layer], axis =-1)\n",
    "\n",
    "    # additional feed-forward layers\n",
    "    both_layer = SkipCon(size = 64, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(combined)\n",
    "    both_layer = keras.layers.Dropout(0.5)(both_layer)\n",
    "    both_layer = SkipCon(size = 16, deep = 2, reduce = False, skip_when=0, activation=\"relu\")(both_layer)\n",
    "\n",
    "    # output layer\n",
    "    z = keras.layers.Dense(3, activation=\"softmax\")(both_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_user, input_items], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nlp_model():\n",
    "    '''Function that creates and compiles the NLP model'''\n",
    "    \n",
    "    print('loading language model')\n",
    "    #download USE from TensorFlow Hub\n",
    "    embedding = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=params['trainable'] , dtype=tf.string, input_shape=[], output_shape=[512])(input_nlp)\n",
    "    nlp_layer = keras.layers.Dense(256, activation='relu')(embedding)\n",
    "    nlp_layer = keras.layers.Dense(128, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(128, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(64, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(16, activation='relu')(nlp_layer)\n",
    "\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(3, activation=\"softmax\")(nlp_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_nlp], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_model():\n",
    "    '''Function that creates and compiles the combined neural network (categorical, numeric and text features)'''\n",
    "    #Customer pathway\n",
    "    user_layer = keras.layers.Dense(256, activation='relu', use_bias = True)(feature_layer_users)\n",
    "    user_layer = SkipCon(size = 256, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(user_layer)\n",
    "    user_layer = keras.layers.Dropout(0.5)(user_layer)\n",
    "    user_layer = SkipCon(size = 256, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(user_layer)\n",
    "    user_layer = keras.layers.Dropout(0.5)(user_layer)\n",
    "    user_layer = SkipCon(size = 64, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(user_layer)\n",
    "\n",
    "    #Item pathway\n",
    "    item_layer = keras.layers.Dense(256, activation='relu', use_bias = True)(feature_layer_items)\n",
    "    item_layer = SkipCon(size = 256, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(item_layer)\n",
    "    item_layer = keras.layers.Dropout(0.5)(item_layer) # Way to handle overfitting\n",
    "    item_layer = SkipCon(size = 256, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(item_layer)\n",
    "    item_layer = keras.layers.Dropout(0.5)(item_layer)\n",
    "    item_layer = SkipCon(size = 64, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(item_layer)\n",
    "\n",
    "    #Text pathway\n",
    "    print('loading language model')\n",
    "    embedding = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=params['trainable'] , dtype=tf.string, input_shape=[], output_shape=[512])\n",
    "    embedding = embedding(input_nlp)\n",
    "    nlp_layer = keras.layers.Dense(256, activation='relu')(embedding)\n",
    "    nlp_layer = keras.layers.Dense(128, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(128, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(64, activation='relu')(nlp_layer)\n",
    "\n",
    "    # combine the output of the three branches\n",
    "    combined = tf.concat([user_layer, item_layer, nlp_layer], axis =-1)\n",
    "\n",
    "    #additional feed-forward layers\n",
    "    both_layer = SkipCon(size = 64, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(combined)\n",
    "    both_layer = keras.layers.Dropout(0.5)(both_layer)\n",
    "    both_layer = SkipCon(size = 16, deep = 2, reduce = False, skip_when=0, activation=\"relu\")(both_layer)\n",
    "\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(3, activation=\"softmax\")(both_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_user, input_items, input_nlp], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not separated neural networks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_not_separated_embedding_model():\n",
    "    #hidden layers\n",
    "    tab_layer = keras.layers.Dense(512, activation='relu', use_bias = True)(feature_layer_tab)\n",
    "    tab_layer = SkipCon(size = 512, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer) \n",
    "    tab_layer = SkipCon(size = 512, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    tab_layer = SkipCon(size = 128, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = SkipCon(size = 64, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    tab_layer = SkipCon(size = 16, deep = 2, reduce = False, skip_when=0, activation=\"relu\")(tab_layer)\n",
    "\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(3, activation=\"softmax\")(tab_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_tab], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile neural network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_not_separated_one_hot_model():\n",
    "    #hidden layers\n",
    "    tab_layer = keras.layers.Dense(256, activation='relu', use_bias = True)(feature_layer_tab)\n",
    "    tab_layer = SkipCon(size = 256, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    tab_layer = SkipCon(size = 256, deep = 2, reduce = True, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    tab_layer = SkipCon(size = 64, deep = 2, reduce = True, skip_when=0, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = SkipCon(size = 32, deep = 2, reduce = False, skip_when=1, activation=\"relu\")(tab_layer)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    tab_layer = SkipCon(size = 8, deep = 2, reduce = False, skip_when=0, activation=\"relu\")(tab_layer)\n",
    "\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(3, activation=\"softmax\")(tab_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_tab], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile neural network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_history(history):\n",
    "    train_dict=dict()\n",
    "    val_dict=dict()\n",
    "    for (key, value) in history.items():\n",
    "       # Check if key is even then add pair to new dictionary\n",
    "        if key.split('_')[0] == 'val':\n",
    "            val_dict[key] = value\n",
    "        else:\n",
    "            train_dict[key] = value\n",
    "    ordered_history = train_dict.copy()\n",
    "    ordered_history.update(val_dict)\n",
    "    return ordered_history\n",
    "\n",
    "def add_fold_to_dict(history, dt):\n",
    "    history = history.copy()\n",
    "    if dt == {}:\n",
    "        dt = dict(history)\n",
    "    else:\n",
    "        for key in dt.keys():\n",
    "            dt[key].extend(history[key])\n",
    "    return dt\n",
    "\n",
    "def get_avg_column_val(df):\n",
    "    averages = list()\n",
    "    for i in range(len(df.columns)):\n",
    "        averages.append(df[i].mean())\n",
    "    return averages\n",
    "\n",
    "def kfold_results(dt, n_folds, epochs):\n",
    "    averages = dict()\n",
    "    for key in dt.keys():\n",
    "        df = pd.DataFrame(pd.Series(dt[key]).values.reshape(n_folds,epochs))\n",
    "        averages[key] = get_avg_column_val(df)\n",
    "    epochs = np.arange(epochs)\n",
    "    results_lists = {'epochs': epochs}\n",
    "    results_lists.update(averages)\n",
    "    results = pd.DataFrame(results_lists)\n",
    "    return results\n",
    "\n",
    "def plot_kfold_results(results):\n",
    "    nb_epochs=results.shape[0]\n",
    "    fig,ax = plt.subplots(2,1,figsize=(8,12))\n",
    "    ax[0].plot(list(range(nb_epochs)), results['loss'], label='Training loss')\n",
    "    ax[0].plot(list(range(nb_epochs)), results['val_loss'], label='Validation loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['binary_accuracy'], label='Training Accuracy')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['val_binary_accuracy'], label='Validation Accuracy')\n",
    "    ax[1].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy / %')\n",
    "    ax[1].legend(loc='best')\n",
    "\n",
    "def print_results(cv_results, test_results=None):\n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Test Set Results:')\n",
    "    print('\\n')\n",
    "    pprint.pprint(test_results)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Cross-Validation Results (averaged over folds):')\n",
    "    print('\\n')\n",
    "    print(cv_results)\n",
    "    plot_kfold_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels):\n",
    "    '''Return one-hot encoded labels'''\n",
    "    labels = labels.apply(lambda x:x == CLASS_LABELS)\n",
    "    labels *= 1\n",
    "    return labels\n",
    "\n",
    "def calc_metrics(logs:dict, predict_probs, target, validation=False, test=False):\n",
    "    '''\n",
    "    Function to calculate the evaluation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    logs: dict with logs\n",
    "    predict: prediction probabilities\n",
    "    target: target labels\n",
    "    validation: True if needed to calculate validation metrics\n",
    "    '''\n",
    "    \n",
    "    #get actual class prediction and target\n",
    "    predict_classes = np.argmax(predict_probs, axis=1)\n",
    "    target_classes = np.argmax(target, axis=1)\n",
    "    \n",
    "    if validation:\n",
    "        prefix = 'val_'\n",
    "    elif test:\n",
    "        prefix = 'test_'\n",
    "    else:\n",
    "        prefix = '' \n",
    "    metrics = dict()\n",
    "    metrics[prefix + 'f1_micro'] = f1_score(target_classes, predict_classes, average='micro')\n",
    "    metrics[prefix + 'f1_macro'] = f1_score(target_classes, predict_classes, average='macro')\n",
    "    metrics[prefix + 'auc_micro'] = roc_auc_score(target, predict_probs, average='micro')\n",
    "    metrics[prefix + 'auc_macro'] = roc_auc_score(target, predict_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    #prepare print message\n",
    "    message = ''\n",
    "    message = [message + f' - {metric}: {metrics[metric]}' for metric in metrics.keys()]\n",
    "\n",
    "    #add calculated metrics to logs\n",
    "    for metric in metrics.keys():\n",
    "        logs[metric] = metrics[metric]\n",
    "\n",
    "    return logs, message\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    '''Callback to calculate additional metrics'''\n",
    "    \n",
    "    def __init__(self, training_data, train_targ, validation_data, val_targ, verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.training_data = training_data\n",
    "        self.train_targ = train_targ\n",
    "        self.validation_data = validation_data\n",
    "        self.val_targ = val_targ\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        #predict on train set\n",
    "        train_predict_probs = np.asarray(self.model.predict(self.training_data))\n",
    "        #targets\n",
    "        train_target = list(self.train_targ)\n",
    "        #calculate metrics\n",
    "        logs, message_train = calc_metrics(logs, predict_probs=train_predict_probs, target=train_target)\n",
    "        \n",
    "        #metrics for validation set:\n",
    "        val_predict_probs = np.asarray(self.model.predict(self.validation_data))\n",
    "        val_target = list(self.val_targ)\n",
    "        logs, message_val = calc_metrics(logs, predict_probs=val_predict_probs, target=val_target, validation=True)\n",
    "        \n",
    "        if self.verbose==1: print(message_train + message_val)\n",
    "        return\n",
    "\n",
    "class Test_Metrics(Callback):\n",
    "    '''Callback to calculate metrics for test set'''\n",
    "    def __init__(self, test_data, test_targ):\n",
    "        super(Callback, self).__init__()\n",
    "        self.test_data = test_data\n",
    "        self.test_targ = test_targ\n",
    "        self.metrics = dict()\n",
    "        \n",
    "    def on_test_end(self, logs):\n",
    "        #calculate metrics for test set\n",
    "        test_predict_probs = np.asarray(self.model.predict(self.test_data))\n",
    "        test_target = list(self.test_targ)\n",
    "        self.metrics, message = calc_metrics(self.metrics, predict_probs=test_predict_probs, target=test_target, test=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_test(train, test, model_fn, epochs, bs): \n",
    "    '''\n",
    "    Function to fit model on complete training data and evaluate on test data\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    test: testing data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    bs: batch size\n",
    "    \n",
    "    Returns evaluation results and training history\n",
    "    '''\n",
    "    \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    \n",
    "    #get TF datasets from pandas DataFrame\n",
    "    random_seed(123)\n",
    "    train_ds = df_to_dataset(train, shuffle=False, batch_size=bs)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=None)\n",
    "    \n",
    "    #create model\n",
    "    random_seed(123)\n",
    "    model = model_fn()\n",
    "    \n",
    "    #fit model on complete training data\n",
    "    random_seed(123)\n",
    "    history = model.fit(train_ds, validation_data=test_ds, epochs=epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    #evaluate the network with the test set:\n",
    "    test_results = dict()\n",
    "    #get one-hot encoded targets\n",
    "    test_targ = get_labels(test['fit'])\n",
    "    #prepare callback\n",
    "    test_metrics = Test_Metrics(test_ds, test_targ)\n",
    "    #evaluate \n",
    "    evaluation = model.evaluate(test_ds, callbacks=[test_metrics])\n",
    "    #add metrics to test results\n",
    "    for idx, metric in enumerate(model.metrics_names):\n",
    "        test_results['test_' + metric] = evaluation[idx]\n",
    "    test_results.update(test_metrics.metrics)\n",
    "    print(test_results)\n",
    "    \n",
    "    return (test_results, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_cv(train, model_fn, epochs, n_folds, bs, test=None):\n",
    "    '''\n",
    "    Function to fit model on training data with cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    n_folds: number of folds into which the training data should be splitted\n",
    "    bs: batch size\n",
    "    test: optional - if given, additionally, a neural network is trained on the complete training data and evaluated on the test data\n",
    "\n",
    "    \n",
    "    Returns cross-validation results\n",
    "    '''\n",
    "    \n",
    "    train = train.copy()\n",
    "    if test is not None:\n",
    "        test = test.copy()\n",
    "        \n",
    "    #tracking variable\n",
    "    folds = dict()\n",
    "    \n",
    "    #prepare cross validation\n",
    "    random_seed(123)\n",
    "    stratified_k_fold = StratifiedKFold(n_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    #iterate over folds\n",
    "    for iteration_idx, (train_idxs, valid_idxs) in enumerate(stratified_k_fold.split(train.loc[:,train.columns!=dep_name[0]], train[dep_name])):\n",
    "        fold_idx = iteration_idx+1\n",
    "        print('-'*20, '\\n', f'> Fold: {fold_idx}'); print('-'*20)\n",
    "        \n",
    "        #get training and validation sets\n",
    "        train_df = train.iloc[train_idxs]\n",
    "        val_df = train.iloc[valid_idxs]\n",
    "        #transform DataFrames into TF datasets\n",
    "        train_ds = df_to_dataset(train_df, shuffle=False, batch_size=bs)\n",
    "        train_ds_metric = df_to_dataset(train_df, shuffle=False, batch_size=None)\n",
    "        val_ds = df_to_dataset(val_df, shuffle=False, batch_size=None)\n",
    "        \n",
    "        #add additional metrics (callback)\n",
    "        train_labels = train_df['fit']\n",
    "        val_labels = val_df['fit']\n",
    "        metrics = Metrics(training_data=train_ds_metric, train_targ=get_labels(train_labels), validation_data=val_ds, val_targ=get_labels(val_labels), verbose=0)\n",
    "    \n",
    "        #create model\n",
    "        random_seed(123)\n",
    "        model = model_fn()\n",
    "        \n",
    "        #model fitting\n",
    "        random_seed(123)\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[metrics], verbose=0)\n",
    "        history.history = order_history(history.history)\n",
    "        \n",
    "        #add fold to dict\n",
    "        folds = add_fold_to_dict(history.history, folds)\n",
    "        \n",
    "        #clear session\n",
    "        keras.backend.clear_session()\n",
    "    \n",
    "    #get results    \n",
    "    results = kfold_results(folds, n_folds, epochs)  \n",
    "    experiment_results = [results]\n",
    "    \n",
    "    if test is not None:\n",
    "        #train a network on complete training set and evaluate on test set\n",
    "        print('-'*15)\n",
    "        print('Test Set: \\n')\n",
    "        \n",
    "        #create, fit and evaluate network\n",
    "        test_results, history_test = fit_model_test(train, test, params['model_fn'], epochs=epochs, bs=params['bs'])\n",
    "        \n",
    "        #get trained epochs in case network stopped early through callback\n",
    "        final_epochs = epochs if early_stopping.stopped_epoch==0 else (early_stopping.stopped_epoch-early_stopping.patience+1)\n",
    "        \n",
    "        experiment_results.append(test_results)\n",
    "      \n",
    "    #show results\n",
    "    if test is not None:\n",
    "        print_results(experiment_results[0], experiment_results[1])\n",
    "    else:\n",
    "        print_results(experiment_results[0])\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['model_fn'] = create_paper_model\n",
    "_ = fit_model_cv(train, test=test, model_fn=params['model_fn'], epochs=params['epochs'], n_folds=params['n_folds'], bs=params['bs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp model example (change the model function); potentially memory issues when not enough memory for pre-trained model\n",
    "'''\n",
    "params['model_fn'] = create_nlp_model\n",
    "params['epochs'] = 10\n",
    "_=fit_model_cv(train, test=test, model_fn=params['model_fn'], epochs=params['epochs'], n_folds=params['n_folds'], bs=params['bs'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not separated network example (change the model function)\n",
    "#do not forget to uncomment the correct encoding section (input preparation) and use the respective model function\n",
    "'''\n",
    "params['model_fn'] = create_not_separated_embedding_model\n",
    "params['epochs'] = 50\n",
    "_=fit_model_cv(train, test=test, model_fn=params['model_fn'], epochs=params['epochs'], n_folds=params['n_folds'], bs=params['bs'])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

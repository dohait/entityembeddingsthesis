{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.manifold import TSNE\n",
    "plt.rc('figure', figsize = (20, 8))\n",
    "plt.rc('font', size = 14)\n",
    "plt.rc('axes.spines', top = False, right = False)\n",
    "plt.rc('axes', grid = False)\n",
    "plt.rc('axes', facecolor = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running tensorflow 2.1.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'sample_perc':1,\n",
    "          'bs': 128,\n",
    "          'ps': 0.5,\n",
    "          'emb_drop': 0.0,\n",
    "          'epochs': 9, \n",
    "          'n_folds': 5,\n",
    "          'optimizer': 'adam',\n",
    "          'lr': 1e-03,\n",
    "          'wd': 0.0001,\n",
    "          #'trainable': False,\n",
    "          'hidden_layers': [128],\n",
    "          'save_model': False\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pth = 'DATA_PTH'\n",
    "results_pth = 'experiment_results/'\n",
    "if not os.path.exists(results_pth):\n",
    "    os.makedirs(results_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'DEPENDENT_VARIABLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed_value, use_cuda=True):\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    random.seed(seed_value) # Python\n",
    "    tf.random.set_seed(seed_value) #tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sample_perc=1): \n",
    "    '''Load and preprocess data'''\n",
    "    \n",
    "    df = pd.read_csv(dataset_pth + 'DATASET.csv', sep=',')\n",
    "    #remove high cardinal features with over 500 unique values\n",
    "    for col in df.columns:\n",
    "        if (df[col].nunique()>500) & (col!='TEXT_VARIABLE'):\n",
    "            print(col, df[col].nunique())\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    if sample_perc < 1:\n",
    "        #use sample of the data\n",
    "        np.random.seed(123)\n",
    "        sampler = StratifiedShuffleSplit(train_size=sample_perc, n_splits=1)\n",
    "        sample,_ = next(sampler.split(df.loc[:,df.columns!=target], df[target]))\n",
    "        df = df.loc[sample].reset_index(drop=True)\n",
    "        print(f'Using {sample_perc*100}% of the data')\n",
    "        \n",
    "        random_seed(123)\n",
    "        stratified_split = StratifiedKFold(5, shuffle=True, random_state=123)\n",
    "        train_indexes, test_indexes = next(stratified_split.split(df.loc[:,df.columns!=target], df[target]))\n",
    "    else:\n",
    "        #load split indexes\n",
    "        from numpy import genfromtxt\n",
    "        train_indexes = genfromtxt(dataset_pth + 'TRAINING_IDXS.csv', delimiter=',', dtype=float).astype(int)\n",
    "        test_indexes = genfromtxt(dataset_pth + 'TESTING_IDXS.csv', delimiter=',', dtype=float).astype(int)\n",
    "    \n",
    "    #split data into train and test set\n",
    "    train = df.loc[train_indexes]\n",
    "    test = df.loc[test_indexes]\n",
    "    return (df, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, train, test = preprocess_data(params['sample_perc'])\n",
    "df.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign variables to data types\n",
    "cat_vars = ['CAT_VARS']\n",
    "\n",
    "cont_vars = ['CONT_VARS']\n",
    "\n",
    "text_var = 'TEXT_VAR'\n",
    "\n",
    "all_features = cat_vars + cont_vars + [text_var]\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features\n",
    "scaler = preprocessing.MinMaxScaler().fit(train[cont_vars])\n",
    "train[cont_vars] = scaler.transform(train[cont_vars])\n",
    "test[cont_vars] = scaler.transform(test[cont_vars])\n",
    "\n",
    "#Categorify categorical features\n",
    "for col in cat_vars+[text_var, target]:\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    df[col] = df[col].astype(str)\n",
    "    \n",
    "#lowercase the text feature\n",
    "train[text_var] = train[text_var].apply(lambda x: x.lower())\n",
    "test[text_var] = test[text_var].apply(lambda x: x.lower())\n",
    "    \n",
    "CLASS_LABELS =  np.array(df[target].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle:bool=True, batch_size=32):\n",
    "    '''Create tf.data dataset from pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: pandas DataFrame\n",
    "    shuffle: Boolean indicating whether to shuffle the data\n",
    "    batch_size: batch size\n",
    "    \n",
    "    Returns a tf.data dataset\n",
    "    '''\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target)\n",
    "    #create one-hot encoded label vector\n",
    "    labels = labels.apply(lambda x:x == CLASS_LABELS)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    if batch_size == None:\n",
    "        # complete data in 1 batch\n",
    "        ds = ds.batch(len(labels))\n",
    "    else:\n",
    "        ds = ds.batch(batch_size)\n",
    "    del dataframe\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions calculated by fastai rule\n",
    "dimension_limit = 40 #initial: 600\n",
    "print('Embedding sizes based on fastai rule')\n",
    "emb_szs = {}\n",
    "for column in df[cat_vars]:\n",
    "    n_cat = df[column].nunique()\n",
    "    emb_szs[column] = min(dimension_limit, round(1.6 * n_cat**0.56))\n",
    "pprint(emb_szs)\n",
    "params['emb_szs'] = emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dimension of highest cardinal feature to specific value\n",
    "'''\n",
    "emb_szs['C3_ENHANCED_MATERIAL_GROUP'] = 439\n",
    "params['emb_szs'] = emb_szs\n",
    "pprint(emb_szs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature columns and the final input for the neural network\n",
    "\n",
    "# Numeric Columns\n",
    "numeric_columns = {\n",
    "    col : feature_column.numeric_column(col) \\\n",
    "          for col in cont_vars\n",
    "}\n",
    "\n",
    "# Categorical Columns\n",
    "categorical_columns = {\n",
    "    col : feature_column.categorical_column_with_vocabulary_list(col, df[col].unique().tolist()) \\\n",
    "          for col in cat_vars\n",
    "}\n",
    "\n",
    "\n",
    "#embeddings: uncomment this section (and comment section below) to use entity embeddings\n",
    "params['encoding'] = 'Entity Embeddings'\n",
    "for col in categorical_columns:\n",
    "    categorical_columns[col] = feature_column.embedding_column(categorical_columns[col], dimension=emb_szs[col])\n",
    "\n",
    "    \n",
    "#one-hot encoded: uncomment this section (and comment section above) to use one-hot encoding\n",
    "'''\n",
    "# One-Hot Encoding\n",
    "params['encoding'] = 'One-Hot Encoding'\n",
    "for col in categorical_columns:\n",
    "    categorical_columns[col] = feature_column.indicator_column(categorical_columns[col])\n",
    "'''\n",
    "\n",
    "\n",
    "#CATEGORICAL AND NUMERIC INPUTS - uncomment this section to use both (comment sections below)\n",
    "'''\n",
    "#capture all feature columns in one vector\n",
    "feature_columns = list(numeric_columns.values()) + list(categorical_columns.values())\n",
    "\n",
    "#prepare final inputs\n",
    "input_tabular = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_columns.keys()\n",
    "}\n",
    "input_tabular.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in categorical_columns.keys()\n",
    "})\n",
    "'''\n",
    "\n",
    "#ONLY NUMERIC INPUTS - uncomment this section (comment sections above and below)\n",
    "'''\n",
    "feature_columns = list(numeric_columns.values())\n",
    "input_tabular = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_columns.keys()\n",
    "}\n",
    "'''\n",
    "\n",
    "#ONLY CATEGORICAL INPUTS - uncomment this section (comment sections above)\n",
    "feature_columns = list(categorical_columns.values())\n",
    "input_tabular = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in categorical_columns.keys()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#NLP input:\n",
    "input_nlp = tf.keras.layers.Input(name=text_var, shape=(), dtype = tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipCon(keras.layers.Layer):\n",
    "    def __init__(self, size, reduce = True, deep = 3, skip_when=0, activation=\"relu\", kernel_regularizer=0.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Class for skip connections\n",
    "        \n",
    "        @Params\n",
    "        size = size of dense layer\n",
    "        deep = the depth of network in one SkipCon block call\n",
    "        skip_when =  if a skip connection is required, pass 1\n",
    "        activation = by default using relu\n",
    "        \"\"\"    \n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation) \n",
    "        self.main_layers = []\n",
    "        self.skip_when = skip_when \n",
    "        if reduce:\n",
    "            for _ in range(deep):\n",
    "                self.main_layers.extend([\n",
    "                      keras.layers.Dense(size, activation=activation, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(kernel_regularizer)),\n",
    "                      keras.layers.BatchNormalization()])\n",
    "\n",
    "                # Reduce the input size by two each time\n",
    "                size = size/2\n",
    "        else:\n",
    "            for _ in range(deep):\n",
    "                self.main_layers.extend([\n",
    "                    keras.layers.Dense(size, activation=activation, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(kernel_regularizer)),\n",
    "                    keras.layers.BatchNormalization()])\n",
    "\n",
    "        self.skip_layers = []\n",
    "        if skip_when > 0:\n",
    "            if reduce:\n",
    "                size = size*2 # since the size of skipped connection should match with cascaded dense\n",
    "            self.skip_layers = [\n",
    "                    keras.layers.Dense(size, activation=activation,use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(kernel_regularizer)),\n",
    "                    keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        if not self.skip_when:\n",
    "            return self.activation(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare evaluation metrics\n",
    "METRICS = [keras.metrics.TruePositives(name='tp'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.TrueNegatives(name='tn'),\n",
    "           keras.metrics.FalseNegatives(name='fn'), \n",
    "           keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "           keras.metrics.CategoricalAccuracy(name='cat_accuracy'),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),\n",
    "    ]\n",
    "\n",
    "def get_optimizer():\n",
    "    '''Function that returns an optimzer based on the parameters for the model'''\n",
    "    \n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(lr=params['lr'], momentum=params['momentum'], decay=params['wd'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=params['lr'])\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=params['lr'], momentum=params['momentum'])\n",
    "    else:\n",
    "        raise Exception('Wrong input for optimizer parameter given.')\n",
    "    return optimizer\n",
    "\n",
    "#early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=1e-03,\n",
    "        verbose=1,\n",
    "        patience=2,\n",
    "        mode='min',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "#tensorboard callback\n",
    "tensorboard_pth = 'logs//fit//' \n",
    "log_dir = tensorboard_pth + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(tensorboard_pth):\n",
    "    os.makedirs(tensorboard_pth)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch', profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabular_model():\n",
    "    '''Function that creates and compiles the standard neural network (categorical, numeric or both inputs)'''\n",
    "    \n",
    "    # Create a feature layer = input layer\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)(input_tabular)\n",
    "    #hidden layers\n",
    "    tabular_layer = keras.layers.Dense(params['hidden_layers'][0], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(feature_layer)\n",
    "    tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "    if len(params['hidden_layers'])>1:\n",
    "        tabular_layer = keras.layers.Dense(params['hidden_layers'][1], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(tabular_layer)\n",
    "        tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "    if len(params['hidden_layers'])>2:\n",
    "        tabular_layer = keras.layers.Dense(params['hidden_layers'][2], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(tabular_layer)\n",
    "        tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(tabular_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_tabular], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile network\n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabular_skip_model():\n",
    "    '''Function that creates and compiles the standard neural network with skip connections (categorical, numeric or both inputs)'''\n",
    "    \n",
    "    # Create a feature layer = input layer\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)(input_tabular)\n",
    "    #hidden layers\n",
    "    tabular_layer = keras.layers.Dense(params['hidden_layers'][0], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(feature_layer)\n",
    "    tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "    if len(params['hidden_layers'])>1:\n",
    "        tabular_layer = SkipCon(size = params['hidden_layers'][1], deep = 1, reduce = True, skip_when=1, activation=\"relu\", kernel_regularizer=params['wd'])(tabular_layer)\n",
    "        tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "    if len(params['hidden_layers'])>2:\n",
    "        tabular_layer = SkipCon(size = params['hidden_layers'][2], deep = 1, reduce = False, skip_when=1, activation=\"relu\", kernel_regularizer=params['wd'])(tabular_layer)\n",
    "        tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "    #output layer\n",
    "    z = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(tabular_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_tabular], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nlp_model():\n",
    "    '''Function that creates and compiles the NLP network (text input)'''\n",
    "    \n",
    "    #load pre-trained model (USE)\n",
    "    print('loading language model')\n",
    "    embedding = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=params['trainable'] , dtype=tf.string, input_shape=[], output_shape=[512])(input_nlp)\n",
    "    nlp_layer = keras.layers.Dense(256, activation='relu')(embedding)\n",
    "    nlp_layer = keras.layers.Dense(64, activation='relu')(nlp_layer)\n",
    "    nlp_layer = keras.layers.Dense(16, activation='relu')(nlp_layer)\n",
    "    \n",
    "    #output layer\n",
    "    z = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(nlp_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_nlp], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabular_nlp_model():\n",
    "    '''Function that creates and compiles the combined neural network (categorical, (numeric) and text inputs)'''\n",
    "    \n",
    "    # Create a feature layer = input layer for categorical variables (and numeric)\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_columns)(input_tabular)\n",
    "    #hidden layers \n",
    "    tabular_layer = keras.layers.Dense(128, activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(feature_layer)\n",
    "    tabular_layer = keras.layers.Dropout(params['ps'])(tabular_layer)\n",
    "\n",
    "    #load pre-trained NLP model (USE)\n",
    "    print('loading language model')\n",
    "    embedding = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=params['trainable'] , dtype=tf.string, input_shape=[], output_shape=[512])\n",
    "    embedding = embedding(input_nlp)\n",
    "    #hidden layers\n",
    "    nlp_layer = keras.layers.Dense(256, activation='relu')(embedding)\n",
    "    nlp_layer = keras.layers.Dense(128, activation='relu')(nlp_layer)\n",
    "\n",
    "    # combine the output of the two branches\n",
    "    combined = tf.concat([tabular_layer, nlp_layer], axis =-1)\n",
    "\n",
    "    # addition feed-forward layers\n",
    "    both_layer = keras.layers.Dense(64, activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(combined)\n",
    "    both_layer = keras.layers.Dropout(0.5)(both_layer)\n",
    "    both_layer = keras.layers.Dense(32, activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(both_layer)\n",
    "    \n",
    "    #output layer\n",
    "    z = keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\")(both_layer)\n",
    "\n",
    "    random_seed(123)\n",
    "    model = keras.Model(inputs=[input_tabular, input_nlp], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    random_seed(123)\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_history(history):\n",
    "    train_dict=dict()\n",
    "    val_dict=dict()\n",
    "    for (key, value) in history.items():\n",
    "       # Check if key is even then add pair to new dictionary\n",
    "        if key.split('_')[0] == 'val':\n",
    "            val_dict[key] = value\n",
    "        else:\n",
    "            train_dict[key] = value\n",
    "    ordered_history = train_dict.copy()\n",
    "    ordered_history.update(val_dict)\n",
    "    return ordered_history\n",
    "\n",
    "def add_fold_to_dict(history, dt):\n",
    "    history = history.copy()\n",
    "    if dt == {}:\n",
    "        dt = dict(history)\n",
    "    else:\n",
    "        for key in dt.keys():\n",
    "            dt[key].extend(history[key])\n",
    "    return dt\n",
    "\n",
    "def get_avg_column_val(df):\n",
    "    averages = list()\n",
    "    for i in range(len(df.columns)):\n",
    "        averages.append(df[i].mean())\n",
    "    return averages\n",
    "\n",
    "def kfold_results(dt, n_folds, epochs):\n",
    "    averages = dict()\n",
    "    for key in dt.keys():\n",
    "        df = pd.DataFrame(pd.Series(dt[key]).values.reshape(n_folds,epochs))\n",
    "        averages[key] = get_avg_column_val(df)\n",
    "    epochs = np.arange(epochs)\n",
    "    results_lists = {'epochs': epochs}\n",
    "    results_lists.update(averages)\n",
    "    results = pd.DataFrame(results_lists)\n",
    "    return results\n",
    "    \n",
    "def plot_kfold_results(results):\n",
    "    nb_epochs=results.shape[0]\n",
    "    fig,ax = plt.subplots(2,1,figsize=(8,12))\n",
    "    fig.suptitle('Results - averaged over folds')\n",
    "    ax[0].plot(list(range(nb_epochs)), results['loss'], label='Training loss')\n",
    "    ax[0].plot(list(range(nb_epochs)), results['val_loss'], label='Validation loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['cat_accuracy'], label='Training Accuracy')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['val_cat_accuracy'], label='Validation Accuracy')\n",
    "    ax[1].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy / %')\n",
    "    ax[1].legend(loc='best')\n",
    "\n",
    "def print_results(cv_results, test_results=None):\n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Test Set Results:')\n",
    "    print('\\n')\n",
    "    pprint(test_results)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Cross-Validation Results (averaged over folds):')\n",
    "    print('\\n')\n",
    "    print(cv_results)\n",
    "    plot_kfold_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels):\n",
    "    '''Return one-hot encoded labels'''\n",
    "    labels = labels.apply(lambda x:x == CLASS_LABELS)\n",
    "    labels *= 1\n",
    "    return labels\n",
    "\n",
    "def calc_metrics(logs:dict, predict_probs, target, validation=False, test=False):\n",
    "    '''\n",
    "    Function to calculate the evaluation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    logs: dict with logs\n",
    "    predict: prediction probabilities\n",
    "    target: target labels\n",
    "    validation: True if needed to calculate validation metrics\n",
    "    '''\n",
    "    \n",
    "    #get actual class prediction and target\n",
    "    predict_classes = np.argmax(predict_probs, axis=1)\n",
    "    target_classes = np.argmax(target, axis=1)\n",
    "    \n",
    "    if validation:\n",
    "        prefix = 'val_'\n",
    "    elif test:\n",
    "        prefix = 'test_'\n",
    "        #create confusion matrix\n",
    "        conf_targets = [CLASS_LABELS[target] for target in target_classes]\n",
    "        conf_predict = [CLASS_LABELS[predict] for predict in predict_classes]            \n",
    "        conf_matrix = confusion_matrix(conf_targets, conf_predict, labels=CLASS_LABELS)\n",
    "        heat_plot = sns.heatmap(conf_matrix, annot=True, cmap='coolwarm', xticklabels=CLASS_LABELS.astype(float).astype(int).astype(str), yticklabels=CLASS_LABELS.astype(float).astype(int).astype(str))\n",
    "        figure = heat_plot.get_figure()\n",
    "        figure.savefig(results_pth  + 'confusion_matrix.png')\n",
    "    else:\n",
    "        prefix = '' \n",
    "    metrics = dict()\n",
    "    metrics[prefix + 'f1_micro'] = f1_score(target_classes, predict_classes, average='micro')\n",
    "    metrics[prefix + 'f1_macro'] = f1_score(target_classes, predict_classes, average='macro')\n",
    "    metrics[prefix + 'auc_micro'] = roc_auc_score(target, predict_probs, average='micro')\n",
    "    metrics[prefix + 'auc_macro'] = roc_auc_score(target, predict_probs, average='macro', multi_class='ovr')\n",
    "\n",
    "    #prepare print message\n",
    "    message = ''\n",
    "    message = [message + f' - {metric}: {metrics[metric]}' for metric in metrics.keys()]\n",
    "\n",
    "    #add calculated metrics to logs\n",
    "    for metric in metrics.keys():\n",
    "        logs[metric] = metrics[metric]\n",
    "\n",
    "    return logs, message\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    '''Callback to calculate additional metrics'''\n",
    "    \n",
    "    def __init__(self, training_data, train_targ, validation_data, val_targ):\n",
    "        super(Callback, self).__init__()\n",
    "        self.training_data = training_data\n",
    "        self.train_targ = train_targ\n",
    "        self.validation_data = validation_data\n",
    "        self.val_targ = val_targ\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        #predict on train set\n",
    "        train_predict_probs = np.asarray(self.model.predict(self.training_data))\n",
    "        #targets\n",
    "        train_target = list(self.train_targ)\n",
    "        #calculate metrics\n",
    "        logs, message_train = calc_metrics(logs, predict_probs=train_predict_probs, target=train_target)\n",
    "        \n",
    "        #clear memory\n",
    "        for i in range(10):\n",
    "            gc.collect()\n",
    "            \n",
    "        #metrics for validation set:\n",
    "        val_predict_probs = np.asarray(self.model.predict(self.validation_data))\n",
    "        val_target = list(self.val_targ)\n",
    "        logs, message_val = calc_metrics(logs, predict_probs=val_predict_probs, target=val_target, validation=True)\n",
    "        \n",
    "        print(message_train + message_val)\n",
    "        return\n",
    "\n",
    "class Test_Metrics(Callback):\n",
    "    '''Callback to calculate metrics for test set'''\n",
    "    \n",
    "    def __init__(self, test_data, test_targ):\n",
    "        super(Callback, self).__init__()\n",
    "        self.test_data = test_data\n",
    "        self.test_targ = test_targ\n",
    "        self.metrics = dict()\n",
    "        \n",
    "    def on_test_end(self, logs):\n",
    "        #calculate metrics for test set\n",
    "        test_predict_probs = np.asarray(self.model.predict(self.test_data))\n",
    "        test_target = list(self.test_targ)\n",
    "        self.metrics, message = calc_metrics(self.metrics, predict_probs=test_predict_probs, target=test_target, test=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_test(train, test, model_fn, epochs, bs): \n",
    "    '''\n",
    "    Function to fit model on complete training data and evaluate on test data\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    test: testing data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    bs: batch size\n",
    "    \n",
    "    Returns evaluation results and training history\n",
    "    '''\n",
    "    \n",
    "    #get TF datasets from pandas DataFrame\n",
    "    random_seed(123)\n",
    "    train_ds = df_to_dataset(train, shuffle=False, batch_size=bs)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=None)\n",
    "    \n",
    "    #create model\n",
    "    random_seed(123)\n",
    "    model = model_fn()\n",
    "    \n",
    "    #fit model on complete training data\n",
    "    random_seed(123)\n",
    "    history = model.fit(train_ds, validation_data=test_ds, epochs=epochs, callbacks=[early_stopping])\n",
    "    #history = model.fit(train_ds, validation_data=test_ds, epochs=epochs, callbacks=[early_stopping, tensorboard_callback])\n",
    "    \n",
    "    #evaluate the network with the test set:\n",
    "    test_results = dict()\n",
    "    #get one-hot encoded targets\n",
    "    test_targ = get_labels(test[target])\n",
    "    #prepare callback\n",
    "    test_metrics = Test_Metrics(test_ds, test_targ)\n",
    "    #evaluate\n",
    "    evaluation = model.evaluate(test_ds, callbacks=[test_metrics])\n",
    "    #add metrics to test results\n",
    "    for idx, metric in enumerate(model.metrics_names):\n",
    "        test_results['test_' + metric] = evaluation[idx]\n",
    "    test_results.update(test_metrics.metrics)\n",
    "    print(test_results)\n",
    "    \n",
    "    #save model\n",
    "    if params['save_model']:\n",
    "        model.save(results_pth + 'model')\n",
    "    \n",
    "    #clear session\n",
    "    keras.backend.clear_session()\n",
    "    return (test_results, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_cv(train, model_fn, epochs, n_folds, bs, test=None, elongation=None):\n",
    "    '''\n",
    "    Function to fit model on training data with cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    n_folds: number of folds into which the training data should be splitted\n",
    "    bs: batch size\n",
    "    test: optional - if given, additionally, a neural network is trained on the complete training data and evaluated on the test data\n",
    "    '''\n",
    "        \n",
    "    #tracking variable\n",
    "    folds = dict()\n",
    "    \n",
    "    #prepare cross validation\n",
    "    random_seed(123)\n",
    "    stratified_k_fold = StratifiedKFold(n_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    #iterate over folds\n",
    "    for iteration_idx, (train_idxs, valid_idxs) in enumerate(stratified_k_fold.split(train.loc[:,train.columns!=target], train[target])):\n",
    "        fold_idx = iteration_idx+1\n",
    "        print('-'*20, '\\n', f'> Fold: {fold_idx}'); print('-'*20)\n",
    "        \n",
    "        #get training and validation sets\n",
    "        train_df = train.iloc[train_idxs].copy()\n",
    "        val_df = train.iloc[valid_idxs].copy()\n",
    "        #transform DataFrames into TF datasets\n",
    "        train_ds = df_to_dataset(train_df, shuffle=False, batch_size=bs)\n",
    "        val_ds = df_to_dataset(val_df, shuffle=False, batch_size=bs*2)\n",
    "        \n",
    "        #add additional metrics  (callback)\n",
    "        train_labels = train_df[target]\n",
    "        val_labels = val_df[target]\n",
    "        metrics = Metrics(training_data=train_ds, train_targ=get_labels(train_labels), validation_data=val_ds, val_targ=get_labels(val_labels))\n",
    "    \n",
    "        #get model\n",
    "        random_seed(123)\n",
    "        model = model_fn()\n",
    "        if fold_idx == 1: print(model.summary())\n",
    "        \n",
    "        #model fitting\n",
    "        random_seed(123)\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[metrics])\n",
    "        #history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[metrics, tensorboard_callback])\n",
    "        history.history = order_history(history.history)\n",
    "        \n",
    "        #add fold to dict\n",
    "        folds = add_fold_to_dict(history.history, folds)\n",
    "        \n",
    "        #free-up memory\n",
    "        keras.backend.clear_session()\n",
    "        for i in range(30):\n",
    "            gc.collect()\n",
    "        del model\n",
    "        del history\n",
    "        del metrics\n",
    "        del train_df\n",
    "        del train_ds\n",
    "        del val_df\n",
    "        del val_ds\n",
    "    \n",
    "    #get results    \n",
    "    results = kfold_results(folds, n_folds, epochs)\n",
    "    experiment_results = [results]\n",
    "    \n",
    "    if test is not None:\n",
    "         #train a network on complete training set and evaluate on test set\n",
    "        print('-'*15)\n",
    "        print('Test Set: \\n')\n",
    "        \n",
    "        #create, fit and evaluate network\n",
    "        test_results, history_test = fit_model_test(train, test, params['model_fn'], epochs=epochs, bs=params['bs'])\n",
    "        \n",
    "        #get trained epochs in case network stopped early through callback\n",
    "        final_epochs = epochs if early_stopping.stopped_epoch==0 else (early_stopping.stopped_epoch-early_stopping.patience+1)\n",
    "        print('Early Stopping:')\n",
    "        print(early_stopping.stopped_epoch)\n",
    "        \n",
    "        experiment_results.append(test_results)\n",
    "        \n",
    "    #show results\n",
    "    if test is not None:\n",
    "        print_results(experiment_results[0], experiment_results[1])\n",
    "    else:\n",
    "        print_results(experiment_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params['model_fn'] = create_tabular_model\n",
    "params['epochs'] = 9\n",
    "fit_model_cv(train, test=test, model_fn=params['model_fn'], epochs=params['epochs'], n_folds=params['n_folds'], bs=params['bs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import pprint\n",
    "#import glob\n",
    "#import sys\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rc('figure', figsize = (20, 8))\n",
    "plt.rc('font', size = 14)\n",
    "plt.rc('axes.spines', top = False, right = False)\n",
    "plt.rc('axes', grid = False)\n",
    "plt.rc('axes', facecolor = 'white')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'sample_perc':1,\n",
    "          'bs': 16,\n",
    "          'epochs': 11, \n",
    "          'n_folds': 5,\n",
    "          'optimizer': 'adam',\n",
    "          'lr': 5e-03,\n",
    "          #'momentum': 0.9,\n",
    "          'wd': 0.01,\n",
    "          'seed': 123,\n",
    "          'hidden_layers': [16],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'heart_disease_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed_value):\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    random.seed(seed_value) # Python\n",
    "    tf.random.set_seed(seed_value) #TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(seed=123):\n",
    "    '''Load data and split into train and test set'''\n",
    "    \n",
    "    df = pd.read_csv(data_dir + 'heart.csv')    \n",
    "    random_seed(seed)\n",
    "    stratified_split = StratifiedKFold(5, shuffle=True, random_state=seed)\n",
    "    train_indexes, test_indexes = next(stratified_split.split(df.loc[:,df.columns!=dep_name[0]], df[dep_name[0]]))\n",
    "    train = df.loc[train_indexes]\n",
    "    test = df.loc[test_indexes]\n",
    "    print(df.shape, train.shape, test.shape)    \n",
    "    return (df, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__ #tf 2.1.0 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocate variables to data types\n",
    "cat_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "cont_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "dep_name = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df, train, test = load_dataset(params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 indicates no heart disease and 0 indicates heart disease\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features\n",
    "scaler = preprocessing.MinMaxScaler().fit(train[cont_vars])\n",
    "train[cont_vars] = scaler.transform(train[cont_vars])\n",
    "test[cont_vars] = scaler.transform(test[cont_vars])\n",
    "\n",
    "# Categorify categorical features\n",
    "for col in cat_vars:\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    df[col] = df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle:bool=True, batch_size=32):\n",
    "    '''Create tf.data dataset from pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: pandas DataFrame\n",
    "    shuffle: Boolean indicating whether to shuffle the data\n",
    "    batch_size: batch size\n",
    "    \n",
    "    Returns a tf.data dataset\n",
    "    '''\n",
    "    \n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(dep_name[0])\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        random_seed(params['seed'])\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    if batch_size == None:\n",
    "        # complete data in 1 batch\n",
    "        ds = ds.batch(len(labels))\n",
    "    else:\n",
    "        ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define embedding dimensions either through setting all variable's dimensions to one value\n",
    "#or through calculating the dimensions based on the fastai rule\n",
    "print('Entity Embedding Dimensions:')\n",
    "emb_szs = {}\n",
    "for column in df[cat_vars]:\n",
    "    n_cat = df[column].nunique()\n",
    "    emb_szs[column] = min(600, round(1.6 * n_cat**0.56)) #fastai rule\n",
    "    #emb_szs[column] = 5\n",
    "    print(f'{column}: {n_cat} classes -> {emb_szs[column]} dimensions')\n",
    "params['emb_szs'] = emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature columns and the final input for the neural network\n",
    "\n",
    "# Numeric Columns\n",
    "numeric_columns = {\n",
    "    col : feature_column.numeric_column(col) \\\n",
    "          for col in cont_vars\n",
    "}\n",
    "\n",
    "# Categorical Columns\n",
    "categorical_columns = {\n",
    "    col: feature_column.categorical_column_with_vocabulary_list(col, df[col].unique().tolist()) \\\n",
    "        for col in cat_vars\n",
    "}\n",
    "\n",
    "#Embeddings\n",
    "params['encoding'] = 'Entity Embeddings'\n",
    "for col in categorical_columns:\n",
    "    categorical_columns[col] = feature_column.embedding_column(categorical_columns[col], dimension=emb_szs[col])\n",
    "    \n",
    "'''\n",
    "#One Hot encoding\n",
    "params['encoding'] = 'One-Hot Encoding'\n",
    "for col in categorical_columns:\n",
    "    categorical_columns[col] = feature_column.indicator_column(categorical_columns[col])\n",
    "'''\n",
    "\n",
    "# Prepare the final inputs: uncomment the section of the inputs that should be used for the NN\n",
    "\n",
    "# Combined inputs (categorical and numeric)\n",
    "#uncomment this section to  use categorical and numeric inputs\n",
    "'''\n",
    "input_tab = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_columns.keys()\n",
    "}\n",
    "input_tab.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in categorical_columns.keys()\n",
    "})\n",
    "\n",
    "#capture all feature columns in 1 vector\n",
    "feat_cols = list(numeric_columns.values()) + list(categorical_columns.values())\n",
    "'''\n",
    "\n",
    "\n",
    "# Separated inputs (either numeric or categorical)\n",
    "#only numeric inputs\n",
    "#uncomment this section to only use numeric inputs\n",
    "'''\n",
    "feat_cols = list(numeric_columns.values())\n",
    "input_tab = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_columns.keys()\n",
    "}\n",
    "'''\n",
    "\n",
    "#only categorical inputs\n",
    "#uncomment this section to only use categorical inputs\n",
    "feat_cols = list(categorical_columns.values())\n",
    "input_tab = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in categorical_columns.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare evaluation metrics\n",
    "METRICS = [keras.metrics.TruePositives(name='tp'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.TrueNegatives(name='tn'),\n",
    "           keras.metrics.FalseNegatives(name='fn'), \n",
    "           keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    "\n",
    "def get_optimizer():\n",
    "    '''Function that returns an optimzer based on the parameters for the model'''\n",
    "    \n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(lr=params['lr'], momentum=params['momentum'], decay=params['wd'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=params['lr'])\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=params['lr'], momentum=params['momentum'])\n",
    "    else:\n",
    "        raise Exception('Wrong input for optimizer parameter given.')\n",
    "    return optimizer\n",
    "\n",
    "#early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        mode='min',\n",
    "        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tab_model():\n",
    "    '''Function that creates and compiles the neural network'''\n",
    "    #create a feature layer\n",
    "    feature_layer_tab = keras.layers.DenseFeatures(feat_cols)(input_tab)\n",
    "    \n",
    "    #create hidden layers\n",
    "    tab_layer = keras.layers.Dense(params['hidden_layers'][0], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(feature_layer_tab)\n",
    "    tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    if len(params['hidden_layers'])>1:\n",
    "        tab_layer = keras.layers.Dense(params['hidden_layers'][1], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(tab_layer)\n",
    "        tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "    if len(params['hidden_layers'])>2:\n",
    "        tab_layer = keras.layers.Dense(params['hidden_layers'][2], activation='relu', use_bias = True, kernel_regularizer=tf.keras.regularizers.l2(params['wd']))(tab_layer)\n",
    "        tab_layer = keras.layers.Dropout(0.5)(tab_layer)\n",
    "        \n",
    "    #create output layer\n",
    "    z = keras.layers.Dense(1, activation=\"sigmoid\")(tab_layer)\n",
    "\n",
    "    random_seed(params['seed'])\n",
    "    model = keras.Model(inputs=[input_tab], outputs=z)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "    \n",
    "    #compile neural network\n",
    "    random_seed(params['seed'])\n",
    "    model.compile(optimizer= optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_tab_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_history(history):\n",
    "    train_dict=dict()\n",
    "    val_dict=dict()\n",
    "    for (key, value) in history.items():\n",
    "       # Check if key is even then add pair to new dictionary\n",
    "        if key.split('_')[0] == 'val':\n",
    "            val_dict[key] = value\n",
    "        else:\n",
    "            train_dict[key] = value\n",
    "    ordered_history = train_dict.copy()\n",
    "    ordered_history.update(val_dict)\n",
    "    return ordered_history\n",
    "\n",
    "def add_fold_to_dict(history, dt):\n",
    "    history = history.copy()\n",
    "    if dt == {}:\n",
    "        dt = dict(history)\n",
    "    else:\n",
    "        for key in dt.keys():\n",
    "            dt[key].extend(history[key])\n",
    "    return dt\n",
    "\n",
    "def get_avg_column_val(df):\n",
    "    averages = list()\n",
    "    for i in range(len(df.columns)):\n",
    "        averages.append(df[i].mean())\n",
    "    return averages\n",
    "\n",
    "def kfold_results(dt, n_folds, epochs):\n",
    "    averages = dict()\n",
    "    for key in dt.keys():\n",
    "        df = pd.DataFrame(pd.Series(dt[key]).values.reshape(n_folds,epochs))\n",
    "        averages[key] = get_avg_column_val(df)\n",
    "    epochs = np.arange(epochs)\n",
    "    results_lists = {'epochs': epochs}\n",
    "    results_lists.update(averages)\n",
    "    results = pd.DataFrame(results_lists)\n",
    "    return results\n",
    "\n",
    "def plot_kfold_results(results):\n",
    "    nb_epochs=results.shape[0]\n",
    "    fig,ax = plt.subplots(2,1,figsize=(8,12))\n",
    "    ax[0].plot(list(range(nb_epochs)), results['loss'], label='Training loss')\n",
    "    ax[0].plot(list(range(nb_epochs)), results['val_loss'], label='Validation loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['binary_accuracy'], label='Training Accuracy')\n",
    "    ax[1].plot(list(range(nb_epochs)),results['val_binary_accuracy'], label='Validation Accuracy')\n",
    "    ax[1].xaxis.set_ticks(np.arange(0,nb_epochs,1))\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy / %')\n",
    "    ax[1].legend(loc='best')\n",
    "\n",
    "def print_results(cv_results, test_results=None):\n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Test Set Results:')\n",
    "    print('\\n')\n",
    "    pprint.pprint(test_results)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*15)\n",
    "    print('Cross-Validation Results (averaged over folds):')\n",
    "    print('\\n')\n",
    "    print(cv_results)\n",
    "    plot_kfold_results(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_test(train, test, model_fn, epochs, bs): \n",
    "    '''\n",
    "    Function to fit model on complete training data and evaluate on test data\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    test: testing data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    bs: batch size\n",
    "    \n",
    "    Returns evaluation results and training history\n",
    "    '''\n",
    "    \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    \n",
    "    #get TF datasets from pandas DataFrame\n",
    "    random_seed(params['seed'])\n",
    "    train_ds = df_to_dataset(train, shuffle=False, batch_size=bs)\n",
    "    test_ds = df_to_dataset(test, shuffle=False, batch_size=None)\n",
    "    \n",
    "    #create and fit model\n",
    "    random_seed(params['seed'])\n",
    "    model = model_fn()\n",
    "    random_seed(params['seed'])\n",
    "    history = model.fit(train_ds, validation_data=test_ds, epochs=epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    #evaluate the network with the test set\n",
    "    test_results = dict()\n",
    "    evaluation = model.evaluate(test_ds, callbacks=[])\n",
    "    for idx, metric in enumerate(model.metrics_names):\n",
    "        test_results['test_' + metric] = evaluation[idx]\n",
    "    print(test_results)\n",
    "    \n",
    "    return (test_results, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_cv(train, model_fn, epochs, n_folds, bs, test=None):\n",
    "    '''\n",
    "    Function to fit model on training data with cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    train: training data as pandas DataFrame\n",
    "    model_fn: function name that creates the neural network\n",
    "    epochs: number of epochs to train\n",
    "    n_folds: number of folds into which the training data should be splitted\n",
    "    bs: batch size\n",
    "    test: optional - if given, additionally, a neural network is trained on the complete training data and evaluated on the test data\n",
    "\n",
    "    \n",
    "    Returns cross-validation results\n",
    "    '''\n",
    "    \n",
    "    train = train.copy()\n",
    "    if test is not None:\n",
    "        test = test.copy()\n",
    "        \n",
    "    #tracking variable\n",
    "    folds = dict()\n",
    "    \n",
    "    #prepare cross validation\n",
    "    random_seed(params['seed'])\n",
    "    stratified_k_fold = StratifiedKFold(n_folds, shuffle=True, random_state=1)\n",
    "    \n",
    "    #iterate over folds\n",
    "    for iteration_idx, (train_idxs, valid_idxs) in enumerate(stratified_k_fold.split(train.loc[:,train.columns!=dep_name[0]], train[dep_name])):\n",
    "        fold_idx = iteration_idx+1\n",
    "        print('-'*20, '\\n', f'> Fold: {fold_idx}'); print('-'*20)\n",
    "        \n",
    "        #get training and validation sets\n",
    "        train_df = train.iloc[train_idxs]\n",
    "        val_df = train.iloc[valid_idxs]\n",
    "        #transform DataFrames into TF datasets\n",
    "        train_ds = df_to_dataset(train_df, shuffle=False, batch_size=bs)\n",
    "        val_ds = df_to_dataset(val_df, shuffle=False, batch_size=None)\n",
    "        \n",
    "        #get model\n",
    "        random_seed(params['seed'])\n",
    "        model = model_fn()\n",
    "        \n",
    "        #fit model\n",
    "        random_seed(params['seed'])\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[], verbose=0)\n",
    "        history.history = order_history(history.history)\n",
    "        \n",
    "        #add fold to dict\n",
    "        folds = add_fold_to_dict(history.history, folds)\n",
    "        \n",
    "        #clear session\n",
    "        keras.backend.clear_session()\n",
    "    \n",
    "    #get results    \n",
    "    results = kfold_results(folds, n_folds, epochs)\n",
    "    experiment_results = [results]\n",
    "    \n",
    "    if test is not None:\n",
    "        #train a network on complete training set and evaluate on test set\n",
    "        print('-'*15)\n",
    "        print('Test Set: \\n')\n",
    "        \n",
    "        #create, fit and evaluate network\n",
    "        test_results, history_test = fit_model_test(train, test, params['model_fn'], epochs=epochs, bs=params['bs'])\n",
    "        \n",
    "        #get trained epochs in case network stopped early through callback\n",
    "        final_epochs = epochs if early_stopping.stopped_epoch==0 else (early_stopping.stopped_epoch-early_stopping.patience+1)\n",
    "        \n",
    "        experiment_results.append(test_results)\n",
    "     \n",
    "    if test is not None:\n",
    "        print_results(experiment_results[0], experiment_results[1])\n",
    "    else:\n",
    "        print_results(experiment_results[0])\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['model_fn'] = create_tab_model\n",
    "_ = fit_model_cv(train, test=test, model_fn=params['model_fn'], epochs=params['epochs'], n_folds=params['n_folds'], bs=params['bs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
